{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple SVM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple SVM Model Results:\n",
      "Train accuracy: 0.5655\n",
      "Test accuracy: 0.4841\n",
      "Train precision: 0.5778\n",
      "Test precision: 0.4903\n",
      "Train recall: 0.5660\n",
      "Test recall: 0.4859\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.85      0.76        13\n",
      "           1       0.50      0.57      0.53        14\n",
      "           2       0.44      0.85      0.58        13\n",
      "           3       0.44      0.57      0.50        14\n",
      "           4       0.73      0.62      0.67        13\n",
      "           5       0.18      0.21      0.19        14\n",
      "           6       0.64      0.69      0.67        13\n",
      "           7       0.48      0.77      0.59        13\n",
      "           8       0.86      0.86      0.86        14\n",
      "           9       0.91      0.71      0.80        14\n",
      "          10       0.81      1.00      0.90        13\n",
      "          11       0.33      0.85      0.48        13\n",
      "          12       0.64      0.69      0.67        13\n",
      "          13       0.50      0.69      0.58        13\n",
      "          14       0.50      0.62      0.55        13\n",
      "          15       0.50      0.54      0.52        13\n",
      "          16       0.76      0.93      0.84        14\n",
      "          17       0.50      0.43      0.46        14\n",
      "          18       0.80      0.62      0.70        13\n",
      "          19       0.39      0.92      0.55        13\n",
      "          20       0.91      0.71      0.80        14\n",
      "          21       0.22      0.46      0.30        13\n",
      "          22       0.69      0.69      0.69        13\n",
      "          23       1.00      0.77      0.87        13\n",
      "          24       0.27      0.21      0.24        14\n",
      "          25       0.00      0.00      0.00        13\n",
      "          26       0.30      0.62      0.40        13\n",
      "          27       0.50      0.43      0.46        14\n",
      "          28       0.44      0.62      0.52        13\n",
      "          29       0.14      0.15      0.15        13\n",
      "          30       0.11      0.14      0.12        14\n",
      "          31       0.40      0.15      0.22        13\n",
      "          32       0.30      0.23      0.26        13\n",
      "          33       0.37      0.54      0.44        13\n",
      "          34       0.50      0.54      0.52        13\n",
      "          35       0.33      0.62      0.43        13\n",
      "          36       0.88      0.54      0.67        13\n",
      "          37       0.40      0.31      0.35        13\n",
      "          38       0.57      0.31      0.40        13\n",
      "          39       0.24      0.46      0.32        13\n",
      "          40       0.56      0.71      0.62        14\n",
      "          41       0.81      1.00      0.90        13\n",
      "          42       0.42      1.00      0.59        13\n",
      "          43       0.23      0.23      0.23        13\n",
      "          44       0.50      0.36      0.42        14\n",
      "          45       0.25      0.21      0.23        14\n",
      "          46       0.45      0.38      0.42        13\n",
      "          47       0.11      0.15      0.13        13\n",
      "          48       0.89      0.62      0.73        13\n",
      "          49       0.67      0.29      0.40        14\n",
      "          50       0.50      0.77      0.61        13\n",
      "          51       0.43      0.46      0.44        13\n",
      "          52       0.24      0.31      0.27        13\n",
      "          53       0.65      0.85      0.73        13\n",
      "          54       0.33      0.07      0.12        14\n",
      "          55       0.77      0.77      0.77        13\n",
      "          56       0.36      0.38      0.37        13\n",
      "          57       0.50      0.36      0.42        14\n",
      "          58       0.50      0.57      0.53        14\n",
      "          59       0.73      0.62      0.67        13\n",
      "          60       0.19      0.36      0.24        14\n",
      "          61       0.53      0.64      0.58        14\n",
      "          62       0.50      0.54      0.52        13\n",
      "          63       0.54      0.93      0.68        14\n",
      "          64       0.42      0.36      0.38        14\n",
      "          65       0.60      0.92      0.73        13\n",
      "          66       0.81      0.93      0.87        14\n",
      "          67       0.42      0.38      0.40        13\n",
      "          68       0.30      0.69      0.42        13\n",
      "          69       0.38      0.38      0.38        13\n",
      "          70       0.70      0.54      0.61        13\n",
      "          71       1.00      1.00      1.00        13\n",
      "          72       0.50      0.69      0.58        13\n",
      "          73       0.48      0.85      0.61        13\n",
      "          74       0.33      0.15      0.21        13\n",
      "          75       0.77      0.71      0.74        14\n",
      "          76       0.73      0.62      0.67        13\n",
      "          77       0.17      0.14      0.15        14\n",
      "          78       0.33      0.14      0.20        14\n",
      "          79       1.00      1.00      1.00        13\n",
      "          80       0.23      0.21      0.22        14\n",
      "          81       0.22      0.15      0.18        13\n",
      "          82       1.00      0.85      0.92        13\n",
      "          83       0.67      0.46      0.55        13\n",
      "          84       0.00      0.00      0.00        13\n",
      "          85       0.85      0.85      0.85        13\n",
      "          86       0.23      0.23      0.23        13\n",
      "          87       0.41      0.64      0.50        14\n",
      "          88       0.21      0.38      0.27        13\n",
      "          89       0.55      0.46      0.50        13\n",
      "          90       1.00      1.00      1.00        13\n",
      "          91       0.57      1.00      0.72        13\n",
      "          92       0.52      0.86      0.65        14\n",
      "          93       1.00      1.00      1.00        13\n",
      "          94       0.89      0.57      0.70        14\n",
      "          95       0.92      0.92      0.92        13\n",
      "          96       0.53      0.71      0.61        14\n",
      "          97       0.65      1.00      0.79        13\n",
      "          98       0.31      0.31      0.31        13\n",
      "          99       0.33      0.08      0.12        13\n",
      "         100       0.50      0.31      0.38        13\n",
      "         101       0.56      0.64      0.60        14\n",
      "         102       0.82      0.69      0.75        13\n",
      "         103       0.32      0.43      0.36        14\n",
      "         104       0.00      0.00      0.00        14\n",
      "         105       0.25      0.29      0.27        14\n",
      "         106       0.25      0.46      0.32        13\n",
      "         107       0.62      0.36      0.45        14\n",
      "         108       0.40      0.57      0.47        14\n",
      "         109       0.10      0.08      0.09        13\n",
      "         110       0.48      0.92      0.63        13\n",
      "         111       0.00      0.00      0.00        14\n",
      "         112       0.85      0.85      0.85        13\n",
      "         113       0.57      0.31      0.40        13\n",
      "         114       0.91      0.71      0.80        14\n",
      "         115       0.67      0.15      0.25        13\n",
      "         116       0.36      0.29      0.32        14\n",
      "         117       0.81      1.00      0.90        13\n",
      "         118       0.48      0.85      0.61        13\n",
      "         119       0.45      0.36      0.40        14\n",
      "         120       0.46      0.43      0.44        14\n",
      "         121       0.33      0.08      0.12        13\n",
      "         122       0.88      0.50      0.64        14\n",
      "         123       0.57      0.31      0.40        13\n",
      "         124       0.00      0.00      0.00        13\n",
      "         125       0.38      0.36      0.37        14\n",
      "         126       0.62      0.62      0.62        13\n",
      "         127       0.75      0.46      0.57        13\n",
      "         128       0.92      0.92      0.92        13\n",
      "         129       0.10      0.08      0.09        13\n",
      "         130       0.00      0.00      0.00        13\n",
      "         131       0.62      0.71      0.67        14\n",
      "         132       0.20      0.08      0.11        13\n",
      "         133       0.43      0.77      0.56        13\n",
      "         134       0.87      1.00      0.93        13\n",
      "         135       0.44      0.29      0.35        14\n",
      "         136       0.20      0.23      0.21        13\n",
      "         137       0.60      0.21      0.32        14\n",
      "         138       0.32      0.43      0.36        14\n",
      "         139       0.33      0.62      0.43        13\n",
      "         140       0.78      0.50      0.61        14\n",
      "         141       0.24      0.43      0.31        14\n",
      "         142       0.32      0.62      0.42        13\n",
      "         143       0.46      0.46      0.46        13\n",
      "         144       0.44      0.62      0.52        13\n",
      "         145       0.93      1.00      0.96        13\n",
      "         146       0.00      0.00      0.00        13\n",
      "         147       0.33      0.15      0.21        13\n",
      "         148       0.17      0.29      0.21        14\n",
      "         149       0.45      0.69      0.55        13\n",
      "         150       0.33      0.08      0.12        13\n",
      "         151       0.40      0.15      0.22        13\n",
      "         152       0.33      0.31      0.32        13\n",
      "         153       0.14      0.07      0.10        14\n",
      "         154       0.00      0.00      0.00        13\n",
      "         155       0.57      0.62      0.59        13\n",
      "         156       0.39      0.54      0.45        13\n",
      "         157       0.90      0.69      0.78        13\n",
      "         158       0.38      0.23      0.29        13\n",
      "         159       0.40      0.31      0.35        13\n",
      "         160       0.67      0.15      0.25        13\n",
      "         161       0.56      0.38      0.45        13\n",
      "         162       0.38      0.36      0.37        14\n",
      "         163       0.38      0.36      0.37        14\n",
      "         164       0.25      0.07      0.11        14\n",
      "         165       0.89      0.62      0.73        13\n",
      "         166       0.17      0.14      0.15        14\n",
      "         167       0.83      0.71      0.77        14\n",
      "         168       0.50      0.54      0.52        13\n",
      "         169       1.00      0.21      0.35        14\n",
      "         170       0.27      0.21      0.24        14\n",
      "         171       0.00      0.00      0.00        13\n",
      "         172       1.00      0.31      0.47        13\n",
      "         173       0.33      0.07      0.12        14\n",
      "         174       0.00      0.00      0.00        13\n",
      "         175       0.41      0.69      0.51        13\n",
      "         176       0.25      0.14      0.18        14\n",
      "         177       0.50      0.46      0.48        13\n",
      "         178       0.62      0.62      0.62        13\n",
      "         179       0.60      0.46      0.52        13\n",
      "         180       1.00      0.36      0.53        14\n",
      "         181       0.38      0.21      0.27        14\n",
      "\n",
      "    accuracy                           0.48      2429\n",
      "   macro avg       0.49      0.49      0.46      2429\n",
      "weighted avg       0.49      0.48      0.46      2429\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[11  0  0 ...  0  0  0]\n",
      " [ 0  8  0 ...  0  0  0]\n",
      " [ 0  0 11 ...  0  0  0]\n",
      " ...\n",
      " [ 0  1  0 ...  6  0  0]\n",
      " [ 0  0  0 ...  0  5  0]\n",
      " [ 0  0  0 ...  0  0  3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('final_dataset.csv')\n",
    "X = data.iloc[:, :-1].values / 255.0\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create the SVM model\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "accuracies = []\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X_train_scaled, y_train), 1):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    svm_model.fit(X_train_fold, y_train_fold)\n",
    "    y_pred = svm_model.predict(X_val_fold)\n",
    "    \n",
    "    accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    all_y_true.extend(y_val_fold)\n",
    "    all_y_pred.extend(y_pred)\n",
    "    \n",
    "    print(f\"Fold {fold}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_val_fold, y_pred, average='weighted'):.4f}\")\n",
    "    print()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Print overall results\n",
    "print(\"\\nSimple SVM Model Results:\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies):.4f} (+/- {np.std(accuracies):.4f})\")\n",
    "\n",
    "# Calculate and print F1-score, precision, and recall\n",
    "f1 = f1_score(all_y_true, all_y_pred, average='weighted')\n",
    "precision = precision_score(all_y_true, all_y_pred, average='weighted')\n",
    "recall = recall_score(all_y_true, all_y_pred, average='weighted')\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Generate classification report\n",
    "class_names = label_encoder.classes_\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_y_true, all_y_pred, target_names=class_names))\n",
    "\n",
    "# Generate confusion matrix\n",
    "plt.figure(figsize=(20, 16))\n",
    "cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "sns.heatmap(cm, annot=False, cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix (Simple SVM)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Train the final model on the entire training set\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred = svm_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\"Test F1-score: {f1_score(y_test, y_test_pred, average='weighted'):.4f}\")\n",
    "print(f\"Test Precision: {precision_score(y_test, y_test_pred, average='weighted'):.4f}\")\n",
    "print(f\"Test Recall: {recall_score(y_test, y_test_pred, average='weighted'):.4f}\")\n",
    "\n",
    "# Print model complexity (number of support vectors)\n",
    "n_support_vectors = svm_model.n_support_.sum()\n",
    "print(f\"\\nNumber of support vectors: {n_support_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved SVM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "Cross-validation scores: [0.72362326 0.73957797 0.746138   0.73789907 0.73429454]\n",
      "Mean CV score: 0.7363\n",
      "CV score standard deviation: 0.0074\n",
      "\n",
      "Improved SVM Model Results:\n",
      "Best parameters: {'svm__C': 100, 'svm__class_weight': 'balanced', 'svm__degree': 2, 'svm__gamma': 'auto', 'svm__kernel': 'rbf'}\n",
      "Number of features retained after PCA: 7\n",
      "\n",
      "Metrics:\n",
      "Accuracy:\n",
      "  Train: 0.9489\n",
      "  Test:  0.7707\n",
      "  Difference: 0.1782\n",
      "Precision:\n",
      "  Train: 0.9510\n",
      "  Test:  0.7765\n",
      "  Difference: 0.1745\n",
      "Recall:\n",
      "  Train: 0.9490\n",
      "  Test:  0.7714\n",
      "  Difference: 0.1776\n",
      "F1:\n",
      "  Train: 0.9488\n",
      "  Test:  0.7644\n",
      "  Difference: 0.1843\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.92      0.83        13\n",
      "           1       0.80      0.57      0.67        14\n",
      "           2       1.00      0.85      0.92        13\n",
      "           3       0.64      0.64      0.64        14\n",
      "           4       0.47      0.62      0.53        13\n",
      "           5       0.77      0.71      0.74        14\n",
      "           6       0.83      0.77      0.80        13\n",
      "           7       0.72      1.00      0.84        13\n",
      "           8       0.74      1.00      0.85        14\n",
      "           9       1.00      0.86      0.92        14\n",
      "          10       0.87      1.00      0.93        13\n",
      "          11       0.58      0.85      0.69        13\n",
      "          12       0.76      1.00      0.87        13\n",
      "          13       1.00      1.00      1.00        13\n",
      "          14       0.62      0.77      0.69        13\n",
      "          15       0.83      0.77      0.80        13\n",
      "          16       0.88      0.50      0.64        14\n",
      "          17       0.62      0.57      0.59        14\n",
      "          18       0.90      0.69      0.78        13\n",
      "          19       0.68      1.00      0.81        13\n",
      "          20       0.92      0.86      0.89        14\n",
      "          21       0.86      0.46      0.60        13\n",
      "          22       0.81      1.00      0.90        13\n",
      "          23       1.00      1.00      1.00        13\n",
      "          24       0.80      0.86      0.83        14\n",
      "          25       0.64      0.54      0.58        13\n",
      "          26       0.68      1.00      0.81        13\n",
      "          27       0.93      1.00      0.97        14\n",
      "          28       0.75      0.69      0.72        13\n",
      "          29       0.82      0.69      0.75        13\n",
      "          30       0.62      0.57      0.59        14\n",
      "          31       0.55      0.46      0.50        13\n",
      "          32       0.76      1.00      0.87        13\n",
      "          33       0.79      0.85      0.81        13\n",
      "          34       0.80      0.62      0.70        13\n",
      "          35       0.68      1.00      0.81        13\n",
      "          36       0.79      0.85      0.81        13\n",
      "          37       0.75      0.69      0.72        13\n",
      "          38       0.85      0.85      0.85        13\n",
      "          39       0.64      0.54      0.58        13\n",
      "          40       0.76      0.93      0.84        14\n",
      "          41       0.81      1.00      0.90        13\n",
      "          42       0.93      1.00      0.96        13\n",
      "          43       0.46      0.46      0.46        13\n",
      "          44       0.91      0.71      0.80        14\n",
      "          45       0.71      0.71      0.71        14\n",
      "          46       0.75      0.69      0.72        13\n",
      "          47       0.58      0.54      0.56        13\n",
      "          48       1.00      1.00      1.00        13\n",
      "          49       0.62      0.57      0.59        14\n",
      "          50       0.81      1.00      0.90        13\n",
      "          51       0.58      0.54      0.56        13\n",
      "          52       0.32      0.46      0.38        13\n",
      "          53       1.00      1.00      1.00        13\n",
      "          54       0.88      0.50      0.64        14\n",
      "          55       1.00      0.92      0.96        13\n",
      "          56       0.56      0.77      0.65        13\n",
      "          57       0.60      0.64      0.62        14\n",
      "          58       0.60      0.64      0.62        14\n",
      "          59       0.76      1.00      0.87        13\n",
      "          60       0.67      0.86      0.75        14\n",
      "          61       0.71      0.86      0.77        14\n",
      "          62       0.86      0.92      0.89        13\n",
      "          63       0.86      0.86      0.86        14\n",
      "          64       0.59      0.71      0.65        14\n",
      "          65       0.72      1.00      0.84        13\n",
      "          66       0.93      1.00      0.97        14\n",
      "          67       0.79      0.85      0.81        13\n",
      "          68       0.63      0.92      0.75        13\n",
      "          69       0.60      0.46      0.52        13\n",
      "          70       0.91      0.77      0.83        13\n",
      "          71       0.93      1.00      0.96        13\n",
      "          72       0.87      1.00      0.93        13\n",
      "          73       0.87      1.00      0.93        13\n",
      "          74       0.91      0.77      0.83        13\n",
      "          75       0.79      0.79      0.79        14\n",
      "          76       1.00      0.77      0.87        13\n",
      "          77       0.56      0.64      0.60        14\n",
      "          78       0.82      1.00      0.90        14\n",
      "          79       1.00      1.00      1.00        13\n",
      "          80       0.73      0.57      0.64        14\n",
      "          81       0.67      0.62      0.64        13\n",
      "          82       1.00      1.00      1.00        13\n",
      "          83       1.00      0.77      0.87        13\n",
      "          84       0.44      0.54      0.48        13\n",
      "          85       0.87      1.00      0.93        13\n",
      "          86       0.90      0.69      0.78        13\n",
      "          87       0.76      0.93      0.84        14\n",
      "          88       0.73      0.85      0.79        13\n",
      "          89       1.00      1.00      1.00        13\n",
      "          90       1.00      1.00      1.00        13\n",
      "          91       1.00      1.00      1.00        13\n",
      "          92       0.81      0.93      0.87        14\n",
      "          93       1.00      1.00      1.00        13\n",
      "          94       0.82      1.00      0.90        14\n",
      "          95       1.00      1.00      1.00        13\n",
      "          96       0.89      0.57      0.70        14\n",
      "          97       1.00      1.00      1.00        13\n",
      "          98       0.82      0.69      0.75        13\n",
      "          99       0.71      0.38      0.50        13\n",
      "         100       0.85      0.85      0.85        13\n",
      "         101       1.00      0.36      0.53        14\n",
      "         102       1.00      1.00      1.00        13\n",
      "         103       0.85      0.79      0.81        14\n",
      "         104       0.67      0.57      0.62        14\n",
      "         105       0.79      0.79      0.79        14\n",
      "         106       0.86      0.92      0.89        13\n",
      "         107       0.75      0.64      0.69        14\n",
      "         108       0.81      0.93      0.87        14\n",
      "         109       0.69      0.85      0.76        13\n",
      "         110       0.65      0.85      0.73        13\n",
      "         111       0.50      0.71      0.59        14\n",
      "         112       0.93      1.00      0.96        13\n",
      "         113       0.75      0.92      0.83        13\n",
      "         114       0.85      0.79      0.81        14\n",
      "         115       0.86      0.46      0.60        13\n",
      "         116       0.62      0.71      0.67        14\n",
      "         117       1.00      1.00      1.00        13\n",
      "         118       0.87      1.00      0.93        13\n",
      "         119       0.92      0.79      0.85        14\n",
      "         120       0.82      1.00      0.90        14\n",
      "         121       0.86      0.92      0.89        13\n",
      "         122       1.00      1.00      1.00        14\n",
      "         123       0.71      0.77      0.74        13\n",
      "         124       0.62      0.62      0.62        13\n",
      "         125       0.76      0.93      0.84        14\n",
      "         126       0.76      1.00      0.87        13\n",
      "         127       1.00      0.85      0.92        13\n",
      "         128       1.00      0.92      0.96        13\n",
      "         129       0.67      0.46      0.55        13\n",
      "         130       0.44      0.31      0.36        13\n",
      "         131       0.81      0.93      0.87        14\n",
      "         132       0.54      0.54      0.54        13\n",
      "         133       0.86      0.92      0.89        13\n",
      "         134       0.93      1.00      0.96        13\n",
      "         135       0.80      0.86      0.83        14\n",
      "         136       0.73      0.85      0.79        13\n",
      "         137       0.78      0.50      0.61        14\n",
      "         138       0.78      1.00      0.88        14\n",
      "         139       0.69      0.85      0.76        13\n",
      "         140       0.93      0.93      0.93        14\n",
      "         141       0.78      1.00      0.88        14\n",
      "         142       0.48      0.85      0.61        13\n",
      "         143       0.78      0.54      0.64        13\n",
      "         144       0.57      0.62      0.59        13\n",
      "         145       1.00      1.00      1.00        13\n",
      "         146       0.00      0.00      0.00        13\n",
      "         147       0.50      0.46      0.48        13\n",
      "         148       0.36      0.36      0.36        14\n",
      "         149       0.86      0.92      0.89        13\n",
      "         150       0.82      0.69      0.75        13\n",
      "         151       0.59      0.77      0.67        13\n",
      "         152       0.90      0.69      0.78        13\n",
      "         153       0.91      0.71      0.80        14\n",
      "         154       1.00      0.62      0.76        13\n",
      "         155       0.79      0.85      0.81        13\n",
      "         156       0.93      1.00      0.96        13\n",
      "         157       0.83      0.77      0.80        13\n",
      "         158       0.64      0.54      0.58        13\n",
      "         159       0.70      0.54      0.61        13\n",
      "         160       0.55      0.46      0.50        13\n",
      "         161       0.91      0.77      0.83        13\n",
      "         162       0.73      0.79      0.76        14\n",
      "         163       0.87      0.93      0.90        14\n",
      "         164       0.58      0.50      0.54        14\n",
      "         165       1.00      0.77      0.87        13\n",
      "         166       0.64      0.50      0.56        14\n",
      "         167       0.86      0.86      0.86        14\n",
      "         168       0.77      0.77      0.77        13\n",
      "         169       0.67      0.57      0.62        14\n",
      "         170       0.91      0.71      0.80        14\n",
      "         171       0.56      0.38      0.45        13\n",
      "         172       0.91      0.77      0.83        13\n",
      "         173       0.71      0.36      0.48        14\n",
      "         174       0.14      0.08      0.10        13\n",
      "         175       0.89      0.62      0.73        13\n",
      "         176       0.71      0.71      0.71        14\n",
      "         177       0.83      0.77      0.80        13\n",
      "         178       1.00      0.69      0.82        13\n",
      "         179       0.79      0.85      0.81        13\n",
      "         180       0.62      0.57      0.59        14\n",
      "         181       0.89      0.57      0.70        14\n",
      "\n",
      "    accuracy                           0.77      2429\n",
      "   macro avg       0.78      0.77      0.76      2429\n",
      "weighted avg       0.78      0.77      0.76      2429\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[12  0  0 ...  0  0  0]\n",
      " [ 1  8  0 ...  0  0  0]\n",
      " [ 0  0 11 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 11  0  0]\n",
      " [ 0  0  0 ...  0  8  0]\n",
      " [ 0  0  0 ...  0  1  8]]\n",
      "\n",
      "Predicted Emotion: Bright\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('final_dataset.csv')\n",
    "X = data.iloc[:, :-1].values / 255.0\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),  # Keep 95% of variance\n",
    "    ('svm', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "    'svm__kernel': ['rbf', 'poly'],\n",
    "    'svm__degree': [2, 3, 4],  # Only used by poly kernel\n",
    "    'svm__class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    scoring=['accuracy', 'precision_macro', 'recall_macro'],\n",
    "    refit='accuracy'\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the number of features retained by PCA\n",
    "n_features_retained = best_model.named_steps['pca'].n_components_\n",
    "\n",
    "# Perform cross-validation on the best model\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV score: {cv_scores.mean():.4f}\")\n",
    "print(f\"CV score standard deviation: {cv_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on train and test sets\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for train and test sets\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, average='macro'),\n",
    "        'recall': recall_score(y_true, y_pred, average='macro'),\n",
    "        'f1': f1_score(y_true, y_pred, average='macro')\n",
    "    }\n",
    "\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nImproved SVM Model Results:\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Number of features retained after PCA: {n_features_retained}\")\n",
    "print(\"\\nMetrics:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "    print(f\"{metric.capitalize()}:\")\n",
    "    print(f\"  Train: {train_metrics[metric]:.4f}\")\n",
    "    print(f\"  Test:  {test_metrics[metric]:.4f}\")\n",
    "    print(f\"  Difference: {train_metrics[metric] - test_metrics[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# Function to predict emotion for new RGB values\n",
    "def predict_emotion(rgb_values):\n",
    "    rgb_array = np.array(rgb_values).reshape(1, -1) / 255.0\n",
    "    prediction_encoded = best_model.predict(rgb_array)\n",
    "    prediction = label_encoder.inverse_transform(prediction_encoded)\n",
    "    return prediction[0]\n",
    "\n",
    "# Example usage\n",
    "new_rgb = [229, 0, 13, 225, 225, 255, 253, 166, 74]\n",
    "predicted_emotion = predict_emotion(new_rgb)\n",
    "print(f\"\\nPredicted Emotion: {predicted_emotion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even more improved by experiment with Regularazation and Error Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 960 candidates, totalling 4800 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 53\u001b[0m\n\u001b[0;32m     42\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     43\u001b[0m     pipeline,\n\u001b[0;32m     44\u001b[0m     param_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Changed to optimize for F1-score\u001b[39;00m\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Fit the grid search\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Get the best model\u001b[39;00m\n\u001b[0;32m     56\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1572\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('final_dataset.csv')\n",
    "X = data.iloc[:, :-1].values / 255.0\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('svm', SVC(random_state=42, probability=True))\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "    'svm__kernel': ['rbf', 'poly'],\n",
    "    'svm__degree': [2, 3, 4],\n",
    "    'svm__class_weight': [None, 'balanced'],\n",
    "    'svm__coef0': [0.0, 0.1, 0.5, 1.0],  # Regularization for polynomial kernel\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
    "    refit='f1_macro'  # Changed to optimize for F1-score\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the number of features retained by PCA\n",
    "n_features_retained = best_model.named_steps['pca'].n_components_\n",
    "\n",
    "# Perform cross-validation on the best model\n",
    "cv_results = grid_search.cv_results_\n",
    "print(f\"Cross-validation scores:\")\n",
    "print(f\"Accuracy: {cv_results['mean_test_accuracy'][grid_search.best_index_]:.4f} (+/- {cv_results['std_test_accuracy'][grid_search.best_index_]:.4f})\")\n",
    "print(f\"Precision: {cv_results['mean_test_precision_macro'][grid_search.best_index_]:.4f} (+/- {cv_results['std_test_precision_macro'][grid_search.best_index_]:.4f})\")\n",
    "print(f\"Recall: {cv_results['mean_test_recall_macro'][grid_search.best_index_]:.4f} (+/- {cv_results['std_test_recall_macro'][grid_search.best_index_]:.4f})\")\n",
    "print(f\"F1-score: {cv_results['mean_test_f1_macro'][grid_search.best_index_]:.4f} (+/- {cv_results['std_test_f1_macro'][grid_search.best_index_]:.4f})\")\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on train and test sets\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for train and test sets\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, average='macro'),\n",
    "        'recall': recall_score(y_true, y_pred, average='macro'),\n",
    "        'f1': f1_score(y_true, y_pred, average='macro')\n",
    "    }\n",
    "\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nImproved SVM Model Results:\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Number of features retained after PCA: {n_features_retained}\")\n",
    "print(\"\\nMetrics:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "    print(f\"{metric.capitalize()}:\")\n",
    "    print(f\"  Train: {train_metrics[metric]:.4f}\")\n",
    "    print(f\"  Test:  {test_metrics[metric]:.4f}\")\n",
    "    print(f\"  Difference: {train_metrics[metric] - test_metrics[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Learning Curves\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_model, X, y_encoded, cv=5, n_jobs=-1, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), scoring='f1_macro'\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# Class distribution analysis\n",
    "class_distribution = pd.Series(y).value_counts().sort_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Emotion Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Function to predict emotion for new RGB values\n",
    "def predict_emotion(rgb_values):\n",
    "    rgb_array = np.array(rgb_values).reshape(1, -1) / 255.0\n",
    "    prediction_encoded = best_model.predict(rgb_array)\n",
    "    prediction_proba = best_model.predict_proba(rgb_array)\n",
    "    prediction = label_encoder.inverse_transform(prediction_encoded)\n",
    "    return prediction[0], prediction_proba[0]\n",
    "\n",
    "# Example usage\n",
    "new_rgb = [229, 0, 13, 225, 225, 255, 253, 166, 74]\n",
    "predicted_emotion, prediction_proba = predict_emotion(new_rgb)\n",
    "print(f\"\\nPredicted Emotion: {predicted_emotion}\")\n",
    "print(\"Prediction Probabilities:\")\n",
    "for emotion, prob in zip(label_encoder.classes_, prediction_proba):\n",
    "    print(f\"{emotion}: {prob:.4f}\")\n",
    "\n",
    "# Error analysis\n",
    "misclassified = X_test[y_test != y_test_pred]\n",
    "misclassified_true = y_test[y_test != y_test_pred]\n",
    "misclassified_pred = y_test_pred[y_test != y_test_pred]\n",
    "\n",
    "print(\"\\nMisclassified Samples Analysis:\")\n",
    "for i in range(min(10, len(misclassified))):  # Print first 10 misclassifications\n",
    "    true_label = label_encoder.inverse_transform([misclassified_true[i]])[0]\n",
    "    pred_label = label_encoder.inverse_transform([misclassified_pred[i]])[0]\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  True label: {true_label}\")\n",
    "    print(f\"  Predicted label: {pred_label}\")\n",
    "    print(f\"  RGB values: {misclassified[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('final_dataset.csv')\n",
    "X = data.iloc[:, :-1].values / 255.0\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('svm', SVC(random_state=42, probability=True))\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "    'svm__kernel': ['rbf', 'poly'],\n",
    "    'svm__degree': [2, 3, 4],\n",
    "    'svm__class_weight': [None, 'balanced'],\n",
    "    'svm__coef0': [0.0, 0.1, 0.5, 1.0],  # Regularization for polynomial kernel\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
    "    refit='f1_macro'  # Changed to optimize for F1-score\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the number of features retained by PCA\n",
    "n_features_retained = best_model.named_steps['pca'].n_components_\n",
    "\n",
    "# Perform cross-validation on the best model\n",
    "cv_results = grid_search.cv_results_\n",
    "print(f\"Cross-validation scores:\")\n",
    "print(f\"Accuracy: {cv_results['mean_test_accuracy'][grid_search.best_index_]:.4f} (+/- {cv_results['std_test_accuracy'][grid_search.best_index_]:.4f})\")\n",
    "print(f\"Precision: {cv_results['mean_test_precision_macro'][grid_search.best_index_]:.4f} (+/- {cv_results['std_test_precision_macro'][grid_search.best_index_]:.4f})\")\n",
    "print(f\"Recall: {cv_results['mean_test_recall_macro'][grid_search.best_index_]:.4f} (+/- {cv_results['std_test_recall_macro'][grid_search.best_index_]:.4f})\")\n",
    "print(f\"F1-score: {cv_results['mean_test_f1_macro'][grid_search.best_index_]:.4f} (+/- {cv_results['std_test_f1_macro'][grid_search.best_index_]:.4f})\")\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on train and test sets\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for train and test sets\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, average='macro'),\n",
    "        'recall': recall_score(y_true, y_pred, average='macro'),\n",
    "        'f1': f1_score(y_true, y_pred, average='macro')\n",
    "    }\n",
    "\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nImproved SVM Model Results:\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Number of features retained after PCA: {n_features_retained}\")\n",
    "print(\"\\nMetrics:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "    print(f\"{metric.capitalize()}:\")\n",
    "    print(f\"  Train: {train_metrics[metric]:.4f}\")\n",
    "    print(f\"  Test:  {test_metrics[metric]:.4f}\")\n",
    "    print(f\"  Difference: {train_metrics[metric] - test_metrics[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Learning Curves\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_model, X, y_encoded, cv=5, n_jobs=-1, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), scoring='f1_macro'\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# Class distribution analysis\n",
    "class_distribution = pd.Series(y).value_counts().sort_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Emotion Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Function to predict emotion for new RGB values\n",
    "def predict_emotion(rgb_values):\n",
    "    rgb_array = np.array(rgb_values).reshape(1, -1) / 255.0\n",
    "    prediction_encoded = best_model.predict(rgb_array)\n",
    "    prediction_proba = best_model.predict_proba(rgb_array)\n",
    "    prediction = label_encoder.inverse_transform(prediction_encoded)\n",
    "    return prediction[0], prediction_proba[0]\n",
    "\n",
    "# Example usage\n",
    "new_rgb = [229, 0, 13, 225, 225, 255, 253, 166, 74]\n",
    "predicted_emotion, prediction_proba = predict_emotion(new_rgb)\n",
    "print(f\"\\nPredicted Emotion: {predicted_emotion}\")\n",
    "print(\"Prediction Probabilities:\")\n",
    "for emotion, prob in zip(label_encoder.classes_, prediction_proba):\n",
    "    print(f\"{emotion}: {prob:.4f}\")\n",
    "\n",
    "# Error analysis\n",
    "misclassified = X_test[y_test != y_test_pred]\n",
    "misclassified_true = y_test[y_test != y_test_pred]\n",
    "misclassified_pred = y_test_pred[y_test != y_test_pred]\n",
    "\n",
    "print(\"\\nMisclassified Samples Analysis:\")\n",
    "for i in range(min(10, len(misclassified))):  # Print first 10 misclassifications\n",
    "    true_label = label_encoder.inverse_transform([misclassified_true[i]])[0]\n",
    "    pred_label = label_encoder.inverse_transform([misclassified_pred[i]])[0]\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  True label: {true_label}\")\n",
    "    print(f\"  Predicted label: {pred_label}\")\n",
    "    print(f\"  RGB values: {misclassified[i]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
