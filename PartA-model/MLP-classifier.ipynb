{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain parameters for the code above and why i use these values : \n",
    "\n",
    "* Normalize the input data - Κανονικοποιήση\n",
    "\n",
    "RGB values are typically in the range 0-255. Normalizing them to 0-1 can help the model learn more effectively. Πιο συγκεκριμένα η κανονικοποίηση μπορεί να βοηθήσει το μοντέλο να μάθει πιο γρήγορα, καθώς οι αλγόριθμοι βελτιστοποίησης (όπως η gradient descent) λειτουργούν πιο αποτελεσματικά όταν τα δεδομένα είναι κανονικοποιημένα.H κανονικοποίηση μπορεί να οδηγήσει σε καλύτερη απόδοση του μοντέλου, καθώς βοηθά στην αποφυγή προβλημάτων όπως το \"vanishing gradient\" (εξαφάνιση κλίσης) ή το \"exploding gradient\" (έκρηξη κλίσης).Πολλά νευρωνικά δίκτυα χρησιμοποιούν συναρτήσεις ενεργοποίησης όπως η SIGMOID ή tanh, οι οποίες έχουν σχεδιαστεί για να λειτουργούν καλύτερα με εισόδους μεταξύ 0 και 1.\n",
    "\n",
    "- Encode the target value - Κωδικοποιήση \n",
    "\n",
    "Machine learning models, especially neural networks, work with numbers, not text. Encoding transforms these text labels into a numerical format the model can understand and process. For multi-class problems, encoded labels allow the use of appropriate loss functions like categorical cross-entropy.\n",
    "\n",
    "- MLP Model overall structure \n",
    "\n",
    "1. Activation Functions:\n",
    "\n",
    "- ReLU: \n",
    "\n",
    "Η ReLU εισάγει μη γραμμικότητα στο δίκτυο, επιτρέποντάς του να μαθαίνει σύνθετα μοτίβα. Σε αντίθεση με το sigmoid ή το tanh, η ReLU δεν συνθλίβει τις κλίσεις στη θετική περιοχή, επιτρέποντας καλύτερη ροή κλίσης. Η ReLU είναι απλή στον υπολογισμό, γεγονός που επιταχύνει την εκπαίδευση.\n",
    "\n",
    "- Softmax:  \n",
    "\n",
    "Η κατάλληλη επιλογή για προβλήματα ταξινόμησης πολλαπλών κλάσεων, καθώς μετατρέπει τις εξόδους του μοντέλου σε πιθανότητες για κάθε κλάση.\n",
    "\n",
    "\n",
    "2. Regularization Techniques:\n",
    "\n",
    "- L2 Regularization (0.01): \n",
    "\n",
    "Discourages the model from relying too heavily on any single feature. It's like telling the model \"don't put all your eggs in one basket\". Βοηθά στην αποφυγή υπερβολικής προσαρμογής . Η τιμή 0.001 είναι απλά συνήθης\n",
    "\n",
    "- Dropout (0.4): \n",
    "\n",
    "Το Dropout είναι μια τεχνική κανονικοποίησης που απενεργοποιεί τυχαία ένα ποσοστό των νευρώνων κατά την εκπαίδευση. Αυτό βοηθά στην αποφυγή της υπερβολικής προσαρμογής, αναγκάζοντας το μοντέλο να μάθει πιο γενικευμένα χαρακτηριστικά. Η τιμή 0.4 σημαίνει ότι το 40% των νευρώνων θα απενεργοποιούνται τυχαία σε κάθε βήμα εκπαίδευσης.\n",
    "\n",
    "- BatchNormalization: \n",
    "\n",
    "Είναι κι αυτό μια τεχνική κανονικοποιήσης που κανονικοποιεί τις ενεργοποιήσεις των νευρώνων σε κάθε παρτίδα δεδομενων . Βοηθά στην σταθεροποιήση της εκπαίδευσης και επιταγχύνει τη σύγκλιση και μπορεί να βελτιώσει την απόδοση του μοντέλου .\n",
    "\n",
    "3. Optimizer (Adam) and Learning Rate (0.001):\n",
    "\n",
    "- Adam is like a smart teacher that adjusts how big of learning steps to take.\n",
    "\n",
    "- 0.001 is a common starting point - not too fast, not too slow.\n",
    "\n",
    "\n",
    "4. Loss Function (Categorical Cross-Entropy):\n",
    "\n",
    "- This measures how wrong the model's predictions are. It's particularly good for problems with multiple classes like this one.\n",
    "\n",
    "\n",
    "5. Training Parameters:\n",
    "\n",
    "- Batch size (32): Processes 32 samples at a time. It's a balance between speed and memory use. This can herlp stabliize the learning process.\n",
    "\n",
    "- Epochs (300): Maximum number of times to go through the entire dataset.\n",
    "\n",
    "- Validation split (0.2): Uses 20% of data to check how well the model is learning.\n",
    "\n",
    "\n",
    "6. Callbacks:\n",
    "\n",
    "- ReduceLROnPlateau: \n",
    "\n",
    "Μειώνει αυτόματα τον learning rate όταν η απόδοση του μοντέλου στο validation set σταματά να βελτιώνεται. Αυτό μπορεί να βοηθήσει στην αποφυγή υπερβολικής προσαρμογής και στην επίτευξη καλύτερης απόδοσης.\n",
    "\n",
    "- EarlyStopping: \n",
    "\n",
    "Σταματά την εκπαίδευση όταν η απόδοση στο validation set σταματά να βελτιώνεται για ένα ορισμένο αριθμό εποχών (epochs). Αυτό βοηθά στην αποφυγή υπερβολικής προσαρμογής και στην εξοικονόμηση χρόνου εκπαίδευσης.\n",
    "\n",
    "7. Metrics: \n",
    "Because I'm dealing with imbalanced dataset it's better to use more metrisc because accuracy alone can be misleading !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam, SGD, AdamW\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('final_dataset.csv')\n",
    "X = data.iloc[:, :-1].values / 255.0\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "def create_model(activation='relu', optimizer='adam', regularizer='l2', dropout_rate=0.3, learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation=activation, input_shape=(9,), kernel_regularizer=regularizer(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(128, activation=activation, kernel_regularizer=regularizer(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(64, activation=activation, kernel_regularizer=regularizer(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(182, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adamw':\n",
    "        opt = AdamW(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "    return model\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'activation': ['relu', 'tanh', 'leaky_relu'],\n",
    "    'optimizer': ['adam', 'sgd', 'adamw'],\n",
    "    'regularizer': ['l1', 'l2'],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4, 0.5],\n",
    "    'learning_rate': [0.1, 0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "# Custom Grid Search with Cross-Validation\n",
    "def grid_search_cv(param_grid, X, y, n_splits=5):\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    for activation in param_grid['activation']:\n",
    "        for optimizer in param_grid['optimizer']:\n",
    "            for regularizer in param_grid['regularizer']:\n",
    "                for dropout_rate in param_grid['dropout_rate']:\n",
    "                    for learning_rate in param_grid['learning_rate']:\n",
    "                        print(f\"Testing: activation={activation}, optimizer={optimizer}, regularizer={regularizer}, dropout={dropout_rate}, lr={learning_rate}\")\n",
    "                        \n",
    "                        scores = []\n",
    "                        for train_index, val_index in cv.split(X, y_encoded):\n",
    "                            X_train, X_val = X[train_index], X[val_index]\n",
    "                            y_train, y_val = y_categorical[train_index], y_categorical[val_index]\n",
    "                            \n",
    "                            model = create_model(\n",
    "                                activation=activation if activation != 'leaky_relu' else 'relu',\n",
    "                                optimizer=optimizer,\n",
    "                                regularizer=l1 if regularizer == 'l1' else l2,\n",
    "                                dropout_rate=dropout_rate,\n",
    "                                learning_rate=learning_rate\n",
    "                            )\n",
    "                            \n",
    "                            if activation == 'leaky_relu':\n",
    "                                model.layers[0].activation = LeakyReLU(alpha=0.01)\n",
    "                                model.layers[3].activation = LeakyReLU(alpha=0.01)\n",
    "                                model.layers[6].activation = LeakyReLU(alpha=0.01)\n",
    "                            \n",
    "                            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "                            early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "                            \n",
    "                            history = model.fit(\n",
    "                                X_train, y_train,\n",
    "                                epochs=300,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(X_val, y_val),\n",
    "                                callbacks=[reduce_lr, early_stopping],\n",
    "                                class_weight=class_weight_dict,\n",
    "                                verbose=0\n",
    "                            )\n",
    "                            \n",
    "                            val_pred = model.predict(X_val)\n",
    "                            val_pred_classes = np.argmax(val_pred, axis=1)\n",
    "                            val_true_classes = np.argmax(y_val, axis=1)\n",
    "                            score = accuracy_score(val_true_classes, val_pred_classes)\n",
    "                            scores.append(score)\n",
    "                        \n",
    "                        avg_score = np.mean(scores)\n",
    "                        print(f\"Average score: {avg_score}\")\n",
    "                        \n",
    "                        if avg_score > best_score:\n",
    "                            best_score = avg_score\n",
    "                            best_params = {\n",
    "                                'activation': activation,\n",
    "                                'optimizer': optimizer,\n",
    "                                'regularizer': regularizer,\n",
    "                                'dropout_rate': dropout_rate,\n",
    "                                'learning_rate': learning_rate\n",
    "                            }\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "# Perform Grid Search\n",
    "best_params, best_score = grid_search_cv(param_grid, X, y_categorical)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best score:\", best_score)\n",
    "\n",
    "# Create the final model with best hyperparameters\n",
    "final_model = create_model(\n",
    "    activation=best_params['activation'] if best_params['activation'] != 'leaky_relu' else 'relu',\n",
    "    optimizer=best_params['optimizer'],\n",
    "    regularizer=l1 if best_params['regularizer'] == 'l1' else l2,\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    learning_rate=best_params['learning_rate']\n",
    ")\n",
    "\n",
    "if best_params['activation'] == 'leaky_relu':\n",
    "    final_model.layers[0].activation = LeakyReLU(alpha=0.01)\n",
    "    final_model.layers[3].activation = LeakyReLU(alpha=0.01)\n",
    "    final_model.layers[6].activation = LeakyReLU(alpha=0.01)\n",
    "\n",
    "# Perform final training and evaluation\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "histories = []\n",
    "fold_accuracies = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X, y_encoded), 1):\n",
    "    print(f'Fold {fold}')\n",
    "    \n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y_categorical[train_index], y_categorical[val_index]\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    \n",
    "    history = final_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=300,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[reduce_lr, early_stopping],\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    histories.append(history)\n",
    "    fold_accuracies.append(max(history.history['val_accuracy']))\n",
    "\n",
    "# Print average accuracy across folds\n",
    "print(f\"Average accuracy across {n_splits} folds: {np.mean(fold_accuracies):.4f}\")\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history.history['val_accuracy'], label=f'Fold {i+1}')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on the entire dataset\n",
    "y_pred = final_model.predict(X)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_encoded, y_pred_classes))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_encoded, y_pred_classes)\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_encoded, y_pred_classes))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_encoded, y_pred_classes)\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = np.abs(final_model.layers[0].get_weights()[0]).mean(axis=1)\n",
    "feature_names = [f'Feature {i+1}' for i in range(9)]\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding improvements in the MLP model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from sklearn.model_selection import StratifiedKFold  # For stratified k-fold cross-validation\n",
    "from sklearn.preprocessing import LabelEncoder  # For encoding categorical labels\n",
    "from sklearn.utils.class_weight import compute_class_weight  # To handle class imbalance\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  # For model evaluation\n",
    "from tensorflow.keras.models import Sequential  # For creating the neural network model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU  # Neural network layers\n",
    "from tensorflow.keras.optimizers import Adam, SGD, AdamW  # Optimization algorithms\n",
    "from tensorflow.keras.utils import to_categorical  # For one-hot encoding of labels\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler  # Training callbacks\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2  # Regularization techniques\n",
    "from tensorflow.keras.metrics import Precision, Recall  # Additional evaluation metrics\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import seaborn as sns  # For enhanced visualizations\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('final_dataset.csv')  # Load the dataset from a CSV file\n",
    "X = data.iloc[:, :-1].values / 255.0  # Extract features and normalize to [0, 1] range\n",
    "y = data.iloc[:, -1].values  # Extract the target variable (last column)\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()  # Initialize the LabelEncoder\n",
    "y_encoded = label_encoder.fit_transform(y)  # Encode categorical labels to integers\n",
    "y_categorical = to_categorical(y_encoded)  # Convert integer labels to one-hot encoded format\n",
    "\n",
    "# Compute class weights to handle class imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weight_dict = dict(enumerate(class_weights))  # Create a dictionary of class weights\n",
    "\n",
    "# Define the model creation function\n",
    "def create_model(num_layers, neurons_per_layer, activation='relu', optimizer='adam', regularizer='l2', dropout_rate=0.3, learning_rate=0.001):\n",
    "    model = Sequential()  # Initialize a sequential model\n",
    "    \n",
    "    # Add layers to the model based on the specified parameters\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            # Add the input layer with specified number of neurons and activation\n",
    "            model.add(Dense(neurons_per_layer[i], activation=activation, input_shape=(9,), \n",
    "                            kernel_regularizer=regularizer(0.01) if regularizer != 'l1_l2' else l1_l2(l1=0.01, l2=0.01)))\n",
    "        else:\n",
    "            # Add hidden layers\n",
    "            model.add(Dense(neurons_per_layer[i], activation=activation, \n",
    "                            kernel_regularizer=regularizer(0.01) if regularizer != 'l1_l2' else l1_l2(l1=0.01, l2=0.01)))\n",
    "        model.add(BatchNormalization())  # Add batch normalization layer\n",
    "        model.add(Dropout(dropout_rate))  # Add dropout layer for regularization\n",
    "    \n",
    "    model.add(Dense(182, activation='softmax'))  # Add output layer with softmax activation\n",
    "    \n",
    "    # Configure the optimizer based on the specified parameter\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adamw':\n",
    "        opt = AdamW(learning_rate=learning_rate)\n",
    "    \n",
    "    # Compile the model with specified loss function and metrics\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "    return model\n",
    "\n",
    "# Define the hyperparameter grid for search\n",
    "param_grid = {\n",
    "    'num_layers': [2, 3, 4],  # Number of hidden layers to try\n",
    "    'neurons_per_layer': [(256, 128), (256, 128, 64), (512, 256, 128, 64)],  # Neurons in each layer\n",
    "    'activation': ['relu', 'tanh', 'leaky_relu'],  # Activation functions to try\n",
    "    'optimizer': ['adam', 'sgd', 'adamw'],  # Optimization algorithms to try\n",
    "    'regularizer': ['l1', 'l2', 'l1_l2'],  # Regularization techniques to try\n",
    "    'dropout_rate': [0.2, 0.3, 0.4, 0.5],  # Dropout rates to try\n",
    "    'learning_rate': [0.1, 0.01, 0.001, 0.0001]  # Learning rates to try\n",
    "}\n",
    "\n",
    "# Implement a learning rate schedule function\n",
    "def lr_schedule(epoch, initial_lr):\n",
    "    drop = 0.5  # Factor by which the learning rate will be reduced\n",
    "    epochs_drop = 10.0  # Number of epochs after which learning rate is reduced\n",
    "    lr = initial_lr * (drop ** np.floor((1 + epoch) / epochs_drop))  # Calculate new learning rate\n",
    "    return lr\n",
    "\n",
    "# Custom Grid Search with Cross-Validation function\n",
    "def grid_search_cv(param_grid, X, y, n_splits=10):\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)  # Initialize stratified k-fold cross-validation\n",
    "    \n",
    "    # Iterate through all combinations of hyperparameters\n",
    "    for num_layers in param_grid['num_layers']:\n",
    "        for neurons_per_layer in param_grid['neurons_per_layer']:\n",
    "            if len(neurons_per_layer) != num_layers:\n",
    "                continue\n",
    "            for activation in param_grid['activation']:\n",
    "                for optimizer in param_grid['optimizer']:\n",
    "                    for regularizer in param_grid['regularizer']:\n",
    "                        for dropout_rate in param_grid['dropout_rate']:\n",
    "                            for learning_rate in param_grid['learning_rate']:\n",
    "                                print(f\"Testing: layers={num_layers}, neurons={neurons_per_layer}, \"\n",
    "                                      f\"activation={activation}, optimizer={optimizer}, \"\n",
    "                                      f\"regularizer={regularizer}, dropout={dropout_rate}, \"\n",
    "                                      f\"lr={learning_rate}\")\n",
    "                                \n",
    "                                scores = []\n",
    "                                # Perform k-fold cross-validation\n",
    "                                for fold, (train_index, val_index) in enumerate(cv.split(X, y_encoded), 1):\n",
    "                                    X_train, X_val = X[train_index], X[val_index]\n",
    "                                    y_train, y_val = y_categorical[train_index], y_categorical[val_index]\n",
    "                                    \n",
    "                                    # Create and configure the model\n",
    "                                    model = create_model(\n",
    "                                        num_layers=num_layers,\n",
    "                                        neurons_per_layer=neurons_per_layer,\n",
    "                                        activation=activation if activation != 'leaky_relu' else 'relu',\n",
    "                                        optimizer=optimizer,\n",
    "                                        regularizer=l1 if regularizer == 'l1' else (l2 if regularizer == 'l2' else l1_l2),\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        learning_rate=learning_rate\n",
    "                                    )\n",
    "                                    \n",
    "                                    # Apply LeakyReLU activation if specified\n",
    "                                    if activation == 'leaky_relu':\n",
    "                                        for layer in model.layers:\n",
    "                                            if isinstance(layer, Dense):\n",
    "                                                layer.activation = LeakyReLU(alpha=0.01)\n",
    "                                    \n",
    "                                    # Define callbacks for training\n",
    "                                    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001)\n",
    "                                    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "                                    lr_scheduler = LearningRateScheduler(lambda epoch: lr_schedule(epoch, learning_rate))\n",
    "                                    \n",
    "                                    # Train the model\n",
    "                                    history = model.fit(\n",
    "                                        X_train, y_train,\n",
    "                                        epochs=500,\n",
    "                                        batch_size=32,\n",
    "                                        validation_data=(X_val, y_val),\n",
    "                                        callbacks=[reduce_lr, early_stopping, lr_scheduler],\n",
    "                                        class_weight=class_weight_dict,\n",
    "                                        verbose=0\n",
    "                                    )\n",
    "                                    \n",
    "                                    # Monitor for overfitting\n",
    "                                    train_loss = history.history['loss'][-1]\n",
    "                                    val_loss = history.history['val_loss'][-1]\n",
    "                                    if train_loss < 0.3 * val_loss:\n",
    "                                        print(f\"Potential overfitting detected in fold {fold}\")\n",
    "                                    \n",
    "                                    # Evaluate the model\n",
    "                                    val_pred = model.predict(X_val)\n",
    "                                    val_pred_classes = np.argmax(val_pred, axis=1)\n",
    "                                    val_true_classes = np.argmax(y_val, axis=1)\n",
    "                                    score = accuracy_score(val_true_classes, val_pred_classes)\n",
    "                                    scores.append(score)\n",
    "                                    \n",
    "                                    print(f\"Fold {fold} score: {score:.4f}\")\n",
    "                                \n",
    "                                # Calculate average score across folds\n",
    "                                avg_score = np.mean(scores)\n",
    "                                std_score = np.std(scores)\n",
    "                                print(f\"Average score: {avg_score:.4f} (+/- {std_score:.4f})\")\n",
    "                                \n",
    "                                # Update best parameters if current configuration is better\n",
    "                                if avg_score > best_score:\n",
    "                                    best_score = avg_score\n",
    "                                    best_params = {\n",
    "                                        'num_layers': num_layers,\n",
    "                                        'neurons_per_layer': neurons_per_layer,\n",
    "                                        'activation': activation,\n",
    "                                        'optimizer': optimizer,\n",
    "                                        'regularizer': regularizer,\n",
    "                                        'dropout_rate': dropout_rate,\n",
    "                                        'learning_rate': learning_rate\n",
    "                                    }\n",
    "                                    print(\"New best configuration found!\")\n",
    "                                \n",
    "                                # Plot learning curves for the best configuration\n",
    "                                if avg_score == best_score:\n",
    "                                    plt.figure(figsize=(12, 4))\n",
    "                                    plt.subplot(1, 2, 1)\n",
    "                                    plt.plot(history.history['loss'], label='Training Loss')\n",
    "                                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                                    plt.title('Model Loss')\n",
    "                                    plt.xlabel('Epoch')\n",
    "                                    plt.ylabel('Loss')\n",
    "                                    plt.legend()\n",
    "                                    \n",
    "                                    plt.subplot(1, 2, 2)\n",
    "                                    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "                                    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "                                    plt.title('Model Accuracy')\n",
    "                                    plt.xlabel('Epoch')\n",
    "                                    plt.ylabel('Accuracy')\n",
    "                                    plt.legend()\n",
    "                                    \n",
    "                                    plt.tight_layout()\n",
    "                                    plt.show()\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "# Perform Grid Search\n",
    "best_params, best_score = grid_search_cv(param_grid, X, y_categorical)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best score:\", best_score)\n",
    "\n",
    "# Create the final model with best hyperparameters\n",
    "final_model = create_model(\n",
    "    num_layers=best_params['num_layers'],\n",
    "    neurons_per_layer=best_params['neurons_per_layer'],\n",
    "    activation=best_params['activation'] if best_params['activation'] != 'leaky_relu' else 'relu',\n",
    "    optimizer=best_params['optimizer'],\n",
    "    regularizer=l1 if best_params['regularizer'] == 'l1' else (l2 if best_params['regularizer'] == 'l2' else l1_l2),\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    learning_rate=best_params['learning_rate']\n",
    ")\n",
    "\n",
    "# Apply LeakyReLU activation if it was the best activation function\n",
    "if best_params['activation'] == 'leaky_relu':\n",
    "    for layer in final_model.layers:\n",
    "        if isinstance(layer, Dense):\n",
    "            layer.activation = LeakyReLU(alpha=0.01)\n",
    "\n",
    "# Perform final training and evaluation\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "histories = []\n",
    "fold_accuracies = []\n",
    "\n",
    "# Train and evaluate the model using k-fold cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X, y_encoded), 1):\n",
    "    print(f'Fold {fold}')\n",
    "    \n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y_categorical[train_index], y_categorical[val_index]\n",
    "    \n",
    "    # Define callbacks for training\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    lr_scheduler = LearningRateScheduler(lambda epoch: lr_schedule(epoch, best_params['learning_rate']))\n",
    "    \n",
    "    # Train the model\n",
    "    history = final_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=300,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[reduce_lr, early_stopping, lr_scheduler],\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    histories.append(history)\n",
    "    fold_accuracies.append(max(history.history['val_accuracy']))\n",
    "\n",
    "# Print average accuracy across folds\n",
    "print(f\"Average accuracy across {n_splits} folds: {np.mean(fold_accuracies):.4f}\")\n",
    "\n",
    "# Evaluate on the entire dataset\n",
    "y_pred = final_model.predict(X)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_encoded, y_pred_classes))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_encoded, y_pred_classes)\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Nested cross-validation and GridSearchCV as hyperparameter in order to check which is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('final_dataset.csv')\n",
    "X = data.iloc[:, :-1].values / 255.0\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Define the model creation function\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.3, l2_reg=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(X.shape[1],), kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(y_categorical.shape[1], activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrapper for sklearn's GridSearchCV\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'dropout_rate': [0.3, 0.4, 0.5],\n",
    "    'l2_reg': [0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "# Outer cross-validation\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Inner cross-validation\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Nested cross-validation\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(outer_cv.split(X, y_encoded), 1):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y_categorical[train_index], y_categorical[test_index]\n",
    "    \n",
    "    # Perform GridSearchCV on the training data\n",
    "    grid_result = grid_search.fit(X_train, y_train, class_weight=class_weight_dict)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_result.best_estimator_\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    score = best_model.score(X_test, y_test)\n",
    "    cv_scores.append(score)\n",
    "    \n",
    "    print(f\"Fold {fold} - Best parameters: {grid_result.best_params_}, Score: {score:.4f}\")\n",
    "\n",
    "# Print the cross-validation results\n",
    "print(\"\\nCross-validation scores:\", cv_scores)\n",
    "print(f\"Mean CV score: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores) * 2:.4f})\")\n",
    "\n",
    "# Train the final model using the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "final_model = create_model(learning_rate=best_params['learning_rate'],\n",
    "                           dropout_rate=best_params['dropout_rate'],\n",
    "                           l2_reg=best_params['l2_reg'])\n",
    "\n",
    "# Fit the final model on the entire dataset\n",
    "history = final_model.fit(X, y_categorical, epochs=100, batch_size=32, \n",
    "                          validation_split=0.2, class_weight=class_weight_dict, verbose=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Generate predictions on the entire dataset\n",
    "y_pred = final_model.predict(X)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = y_encoded\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
