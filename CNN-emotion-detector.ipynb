{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Emotion Detector\n",
    "\n",
    "##### In this notebook, we will focus on building and training a Convolutional Neural Network (CNN) model to analyze facial expressions in images. \n",
    "##### The CNN is a type of deep learning model that is particularly good at processing images. \n",
    "##### We will train our CNN to recognize different emotions based on facial expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from mtcnn import MTCNN\n",
    "from keras.models import load_model\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the DatasetA - FER2013 dataset and preprocess it. This involves : \n",
    "- converting the pixel values from strings to integers, \n",
    "- reshaping the data into the original image shape, \n",
    "- and normalizing the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('fer2013.csv')\n",
    "\n",
    "# Convert pixels to a numpy array\n",
    "data['pixels'] = data['pixels'].apply(lambda pixel_sequence: np.fromstring(pixel_sequence, sep=' '))\n",
    "\n",
    "# Reshape and normalize the data\n",
    "X = np.vstack(data['pixels'].values)\n",
    "X = X.reshape(-1, 48, 48, 1)\n",
    "X = X.astype('float32')\n",
    "X /= 255.0\n",
    "\n",
    "# Convert labels to a numpy array\n",
    "y = data['emotion'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We laod the DatasetB with the 4000 oil paintings of art portraits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "### Split the Data for the DatasetA\n",
    "We split the data into a training set and a test set. This allows us to evaluate the performance of our model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Train the model \n",
    "We define our CNN model for facial expression recognition and train it on our preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "\n",
    "# Add a pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add another convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Add another pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the tensor output from the previous layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a fully connected layer\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(units=7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model \n",
    "After training, we save our model so that we can use it later without having to retrain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('expression_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Transfer \n",
    "If the paintings are highly stylized or abstract, consider using style transfer to make the FER2013 images look more like paintings. This could potentially improve the performance of the model on paintings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fine-tuning\n",
    "If we have a labeled dataset of paintings with facial expressions, use this to fine-tune your model after pre-training it on the FER2013 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection\n",
    "Implement a face detection system using a model like MTCNN. This system should take a painting as input and output the locations and bounding boxes of any faces in the painting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Prediction\n",
    "- For each new painting, use the face detection system to locate any faces in the painting.\n",
    "- Extract the faces from the painting and preprocess them to match the input that your CNN model expects.\n",
    "- Feed the preprocessed faces into the CNN model to predict the facial expressions.\n",
    "- Postprocess the model's output to get a final emotion prediction for each face. This typically involves taking the emotion with the highest predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "Interpret the model's predictions to determine the overall emotion that the painting is expressing. This could be as simple as taking the most common emotion among all the faces, or it could involve a more complex analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
