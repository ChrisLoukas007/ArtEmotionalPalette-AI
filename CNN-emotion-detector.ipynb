{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Emotion Detector\n",
    "\n",
    "##### In this notebook, we will focus on building and training a Convolutional Neural Network (CNN) model to analyze facial expressions in images. \n",
    "##### The CNN is a type of deep learning model that is particularly good at processing images. \n",
    "##### We will train our CNN to recognize different emotions based on facial expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from mtcnn import MTCNN\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "from keras.preprocessing.image import ImageDataGenerator  # For data augmentation\n",
    "from keras.applications import VGG19  # For style transfer\n",
    "from keras import backend as K  # For defining loss functions in style transfer\n",
    "from keras.optimizers import Adam  # For compiling the model\n",
    "from keras.preprocessing.image import img_to_array, load_img  # For loading and preprocessing images\n",
    "from PIL import Image  # For image manipulation\n",
    "import matplotlib.pyplot as plt  # For plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation - Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the DatasetA - FER2013 dataset and preprocess it. This involves : \n",
    "- converting the pixel values from strings to integers, \n",
    "- reshaping the data into the original image shape, \n",
    "- and normalizing the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Assuming the images are stored in subdirectories for each class\n",
    "train_dir = 'C:/Μαθήματα/Διπλωματική/archive/train'\n",
    "test_dir = 'C:/Μαθήματα/Διπλωματική/archive/test'\n",
    "\n",
    "# For image data, Keras provides a useful tool called ImageDataGenerator , that helps you load and preprocess image data\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Initialize the ImageDataGenerator for training and testing\n",
    "#rescale=1./255 argument tells it to transform every pixel value in the image by dividing it by 255. \n",
    "# This scales the pixel values from a range of 0-255 to a range of 0-1, which is a common preprocessing step in neural network models\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, #Note, we add data augmentation to the training data generator in order to improve the model's generalization\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "# Load the images from the train directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48, 48),  # All images will be resized to 48x48\n",
    "        color_mode=\"grayscale\", # converts them to grayscale\n",
    "        batch_size=32, # groups them into batches of 32\n",
    "        class_mode='categorical') #  treat the labels as categorical data (i.e., one-hot encoded labels)\n",
    "\n",
    "# Load the images from the test directory\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(48, 48),\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We laod the DatasetB with the 4000 oil paintings of art portraits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where the second dataset is stored\n",
    "art_dir = 'path_to_your_art_dataset'\n",
    "\n",
    "# Initialize the ImageDataGenerator for the art dataset\n",
    "# Rescale normalizes the pixel values to be between 0 and 1\n",
    "art_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load the images from the art directory\n",
    "art_generator = art_datagen.flow_from_directory(\n",
    "        art_dir,\n",
    "        target_size=(48, 48),  # All images will be resized to 48x48\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "### Split the Data for the DatasetA\n",
    "We split the data into a training set and a test set. This allows us to evaluate the performance of our model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Train the model \n",
    "We define our CNN model for facial expression recognition and train it on our preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "\n",
    "# Add a pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add another convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Add another pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the tensor output from the previous layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a fully connected layer\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(units=7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model \n",
    "After training, we save our model so that we can use it later without having to retrain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('expression_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Transfer \n",
    "If the paintings are highly stylized or abstract, consider using style transfer to make the FER2013 images look more like paintings. This could potentially improve the performance of the model on paintings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fine-tuning\n",
    "If we have a labeled dataset of paintings with facial expressions, use this to fine-tune your model after pre-training it on the FER2013 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection\n",
    "Implement a face detection system using a model like MTCNN. This system should take a painting as input and output the locations and bounding boxes of any faces in the painting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Prediction\n",
    "- For each new painting, use the face detection system to locate any faces in the painting.\n",
    "- Extract the faces from the painting and preprocess them to match the input that your CNN model expects.\n",
    "- Feed the preprocessed faces into the CNN model to predict the facial expressions.\n",
    "- Postprocess the model's output to get a final emotion prediction for each face. This typically involves taking the emotion with the highest predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "Interpret the model's predictions to determine the overall emotion that the painting is expressing. This could be as simple as taking the most common emotion among all the faces, or it could involve a more complex analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
